ðŸ§  Real-Time Sign Language Recognition (Hybrid AI Model)<br>
ðŸ“Œ Project Overview<br>
This project presents a real-time sign language recognition system that converts hand gestures into text and speech using a hybrid AI approach combining:<br>
Convolutional Neural Networks (CNN)<br>
MediaPipe Hand Landmarks<br>
Machine Learning (Random Forest)<br><br>
The system supports multiple prediction modes and demonstrates improved real-time accuracy using a hybrid model.<br>
ðŸš€ Features<br>
ðŸ”¹ CNN-based gesture recognition<br>
ðŸ”¹ Landmark-based gesture detection<br>
ðŸ”¹ Hybrid AI prediction system<br>
ðŸ”¹ Real-time webcam detection<br>
ðŸ”¹ Word formation from gestures<br>
ðŸ”¹ Text-to-speech output<br>
ðŸ”¹ Mode switching:<br>
Press 1 â†’ CNN Mode<br>
Press 2 â†’ Landmark Mode<br>
Press 3 â†’ Hybrid Mode<br><br>
ðŸ§ª Tech Stack<br>
1. Python<br>
2. TensorFlow / Keras<br>
3. OpenCV<br>
4. MediaPipe<br>
5. Scikit-learn<br>
6. NumPy<br><br>
ðŸ“Š AI Models Used<br>
1. CNN Model<br>
  i. Trained on ~87,000 gesture images<br>
  ii. Image-based classification<br>
2. Landmark Model<br>
  i. 21 hand keypoints (63 features)<br>
  ii. Random Forest classifier<br>
  iii. ~98% dataset accuracy<br>
3. Hybrid Model<br>
  i. Combines CNN + Landmark predictions<br>
  ii. Improves real-time stability<br>
