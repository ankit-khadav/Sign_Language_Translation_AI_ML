ðŸ§  Real-Time Sign Language Recognition (Hybrid AI Model)<br>
ðŸ“Œ Project Overview<br>
This project presents a real-time sign language recognition system that converts hand gestures into text and speech using a hybrid AI approach combining:<br>
Convolutional Neural Networks (CNN)<br>
MediaPipe Hand Landmarks<br>
Machine Learning (Random Forest)<br><br>
The system supports multiple prediction modes and demonstrates improved real-time accuracy using a hybrid model.<br>
ðŸš€ Features<br>
ðŸ”¹ CNN-based gesture recognition<br>
ðŸ”¹ Landmark-based gesture detection<br>
ðŸ”¹ Hybrid AI prediction system<br>
ðŸ”¹ Real-time webcam detection<br>
ðŸ”¹ Word formation from gestures<br>
ðŸ”¹ Text-to-speech output<br>
ðŸ”¹ Mode switching:<br>
Press 1 â†’ CNN Mode<br>
Press 2 â†’ Landmark Mode<br>
Press 3 â†’ Hybrid Mode<br><br>
ðŸ§ª Tech Stack<br>
Python<br>
TensorFlow / Keras<br>
OpenCV<br>
MediaPipe<br>
Scikit-learn<br>
NumPy<br><br>
ðŸ“Š AI Models Used<br>
CNN Model<br>
Trained on ~87,000 gesture images<br>
Image-based classification<br>
Landmark Model<br>
21 hand keypoints (63 features)<br>
Random Forest classifier<br>
~98% dataset accuracy<br>
Hybrid Model<br>
Combines CNN + Landmark predictions<br>
Improves real-time stability<br>
